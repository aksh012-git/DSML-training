# -*- coding: utf-8 -*-
"""case_study_Machine_learning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1n2cSUt979F65m-3Z6X_RiUVXLqRF_56E

The Boston Housing Dataset

The Boston Housing Dataset is a derived from information collected by the U.S. Census Service concerning housing in the area of Boston MA. The following describes the dataset columns:

CRIM - per capita crime rate by town

ZN - proportion of residential land zoned for lots over 25,000 sq.ft.

INDUS - proportion of non-retail business acres per town.

CHAS - Charles River dummy variable (1 if tract bounds river; 0 otherwise)

NOX - nitric oxides concentration (parts per 10 million)

RM - average number of rooms per dwelling

AGE - proportion of owner-occupied units built prior to 1940

DIS - weighted distances to five Boston employment centres

RAD - index of accessibility to radial highways

TAX - full-value property-tax rate per $10,000

PTRATIO - pupil-teacher ratio by town

N - 1000(N - 0.63)^2 where N is the proportion of Non-Americans by town

LSTAT - % lower status of the population

MEDV - Median value of owner-occupied homes in $1000's
"""

#----------------------------------------------------------------------------------------------------------------------------------
import numpy as np
import pandas as pd


#----------------------------------------------------------------------------------------------------------------------------------
#see in dataset no any header or column name availabe so i put column_names
#delimiter=r"\s+" bcz our file is space separated file
column_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'N', 'LSTAT', 'MEDV']
housing = pd.read_csv("housing.csv", header=None, delimiter=r"\s+", names=column_names)



#----------------------------------------------------------------------------------------------------------------------------------
housing.head()



#----------------------------------------------------------------------------------------------------------------------------------
print(np.shape(housing))


#----------------------------------------------------------------------------------------------------------------------------------
print(housing.describe())


#----------------------------------------------------------------------------------------------------------------------------------
#find null value in dataset
housing.isna().sum()

"""# for Example
our dataset is person heights and weight prediction
####suppose  
####        170cm  --  70kg
 ####        160cm  --  60kg
but some cases 160cm height ---- 120kg so 
#this called Outlier data

#chack in our case any outlier dataset or not
"""

#----------------------------------------------------------------------------------------------------------------------------------
import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(25,4))
plt.subplot(1,5,1)  
sns.boxplot(housing['CRIM'])
#see after 30 CRIM all value us outlier
#bcz this value is not a close to a blue line
#you can generate all column bocplot
#search on google many outlier handling technique

#----------------------------------------------------------------------------------------------------------------------------------
#all column name and there data
for k,v in housing.items():
    print(k,v)

#----------------------------------------------------------------------------------------------------------------------------------
import seaborn as sns
import matplotlib.pyplot as plt

fig, axs = plt.subplots(ncols=7, nrows=2, figsize=(25, 10))

index=0
axs = axs.flatten()
for k,v in housing.items():
    sns.boxplot(y=v, ax=axs[index])
    index += 1

#see value of MEDV , some value in blue box and some value vary long-distance form the box this callde outlier

#----------------------------------------------------------------------------------------------------------------------------------
#lets remove features of MEDV columns below 50
(housing['MEDV'] >= 50.0)
#see some value <50 then False and other are True

#----------------------------------------------------------------------------------------------------------------------------------
#make negation of MEDV >= 50
~(housing['MEDV'] >= 50.0)

#mean i put only this tipy of date ther havin <50

#----------------------------------------------------------------------------------------------------------------------------------
housing = housing[~(housing['MEDV'] >= 50.0)]
housing

#----------------------------------------------------------------------------------------------------------------------------------
print(np.shape(housing))

#----------------------------------------------------------------------------------------------------------------------------------
#this is corelation matrix
housing.corr()

#----------------------------------------------------------------------------------------------------------------------------------
#styling in corelation mtrix
plt.figure(figsize=(20, 10))
sns.heatmap(housing.corr().abs(),  annot=True)

#in our datse MEDV is our output column
#then see metrix which column is less correlate with MEDV
#i see MEDV with CHAS is vary less correlate so you can remove also

#----------------------------------------------------------------------------------------------------------------------------------
#in the matrix above it is seemed that TAX and RAD coloumns are highly correlated but LSTAT,INDUS,RM,TAX,NOX,PTRAIO columns have low correlation.
#i put only this column, this highly correlated of MEDV
column_sels = ['LSTAT', 'INDUS', 'NOX', 'PTRATIO', 'RM', 'TAX', 'DIS', 'AGE']
#x input and y output column
x = housing.loc[:,column_sels]
y = housing['MEDV']
x

#----------------------------------------------------------------------------------------------------------------------------------
#in dataset some small and some large value alive so i use min-max scaler 
from sklearn import preprocessing
min_max_scaler = preprocessing.MinMaxScaler()
#pd.DataFrame(data=min_max_scaler.fit_transform(x), give column name)
#you don't write "columns=column_sels" then see what output here
x = pd.DataFrame(data=min_max_scaler.fit_transform(x), columns=column_sels)
x

#----------------------------------------------------------------------------------------------------------------------------------
fig, axs = plt.subplots(ncols=4, nrows=2, figsize=(20, 10))
index = 0
axs = axs.flatten()
for i, k in enumerate(column_sels):
    sns.regplot(y=y, x=x[k], ax=axs[i])
plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=5.0)

#see in regplot and regression line which point close to regression line
#see our output column MEDV in y axis
#all input column in x axis
#see MEDV with LSTAT graph this all points are vary close to regreesion line
#so the impact of LSTAT in MEDV are very high
#you can also see in corelation metrix

#----------------------------------------------------------------------------------------------------------------------------------
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler

train_test_split(x, y)
x_train, x_test, y_train, y_test = train_test_split(x, y, train_size = 0.8, random_state = 20)
scaler = MinMaxScaler((-1,1))
x_train = scaler.fit_transform(x_train)
x_test = scaler.transform(x_test)

#----------------------------------------------------------------------------------------------------------------------------------
from sklearn.linear_model import LinearRegression
lr = LinearRegression() 
#fit the model with train data. 
model = lr.fit(x_train,y_train)

#----------------------------------------------------------------------------------------------------------------------------------
r2_train = model.score(x_train,y_train)
r2_test = model.score(x_test,y_test)

print("R2 Score for train data is ", r2_train)
print("R2 Score for test data is ", r2_test)

#----------------------------------------------------------------------------------------------------------------------------------
linear_coef = model.coef_
linear_coef
#in liner regression y = m1x1 + m2x2 + m3x3 + m4x4 + ......
#so all m1 to m8 value over here

#----------------------------------------------------------------------------------------------------------------------------------
#see many to gather
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR

#----------------------------------------------------------------------------------------------------------------------------------
key = ['linierregrassion','RandomForest','supportVector']
value = [LinearRegression(),RandomForestRegressor(),SVR()]
models = dict(zip(key,value))
print(models)
#see in models to models are ready LinearRegression and RandomForestRegressor

#----------------------------------------------------------------------------------------------------------------------------------
x = {'aksh':1234,'vasu':4321,'ganderi':987,'darshan':7890}
for key,value in x.items():
  print(key,value)

#----------------------------------------------------------------------------------------------------------------------------------
ScoreOfModel = []
z = []
for name,algo in models.items():
    model = algo
    model.fit(x_train,y_train)
    predict  = model.predict(x_test)
    score = model.score(x_test,y_test)
    z.append(name)
    z.append(score)
    ScoreOfModel.append(z)
    z = []


print(ScoreOfModel)