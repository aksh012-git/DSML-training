# -*- coding: utf-8 -*-
"""Ensemble_Learning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YHetPqYLCfPNhBJlM9ILEae19C-oJmh8

**Dataset** : https://archive.ics.uci.edu/ml/datasets/Letter+Recognition

## https://archive.ics.uci.edu/ml/datasets/Letter+Recognition
"""

#--------------------------------------------------------------------------------------------------------------------------
# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import *
from sklearn.preprocessing import *
from sklearn.metrics import *
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC

#add ensemble learning 
from sklearn.ensemble import VotingClassifier

from sklearn import model_selection
from sklearn.linear_model import LogisticRegression
# %matplotlib inline

import seaborn as sns 

np.random.seed(2021)

#--------------------------------------------------------------------------------------------------------------------------

columns = ['letter', 'xbox', 'ybox', 'width', 'height', 'onpix', 'xbar', 'ybar','x2bar', 'y2bar', 'xybar', 'x2ybar', 'xy2bar', 'xedge', 'xedgey', 'yedge', 'yedgex']

#read data
#put column name 
data = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/letter-recognition/letter-recognition.data', header=None, names = columns)

data


#--------------------------------------------------------------------------------------------------------------------------
#plt.show()
#which column how many unique values see in graph
data.nunique().plot(kind="bar")
plt.savefig('graph.png')


#--------------------------------------------------------------------------------------------------------------------------
#x is our input and y is our output
#you can write this way drop['letter'] means x having value only other then "letter" column
#y is our output only one column 
X = data.drop(['letter'], axis=1).values
Y = data['letter'].values
#but see Y is 1D array and X is 2D
#so conver
Y = Y.reshape(-1,1)


#you can write this way also
x = data.iloc[:,1:].values
y = data.iloc[:,0].values.reshape(-1,1)

x.shape, y.shape

X.shape, Y.shape


#--------------------------------------------------------------------------------------------------------------------------
#apply label encoder because our output column is categorical data so convert into numerical data 
from sklearn.preprocessing import LabelEncoder

lc= LabelEncoder()

y = lc.fit_transform(y)
y

print(y)

#--------------------------------------------------------------------------------------------------------------------------

x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)


#--------------------------------------------------------------------------------------------------------------------------

"""### Logistic Regression"""

lr = LogisticRegression(max_iter=100, n_jobs=-1)
lr.fit(x_train, y_train)
y_pred =  lr.predict(x_test)
print("--------------- Logistic Regression Report -----------------------")
print(f"Accuracy : {lr.score(x_test, y_test)}")
print(f"Cohen Kappa : {cohen_kappa_score(y_test, y_pred)}")
print(f"Classification Report :\n{classification_report(y_test, y_pred)}")
print("------------------------------------------------------------------")

#--------------------------------------------------------------------------------------------------------------------------

"""## Decision Tree"""

dt = DecisionTreeClassifier(max_depth=100, random_state=2021)
dt.fit(x_train, y_train)
y_pred =  dt.predict(x_test)
print("--------------- Decision Tree Report -----------------------")
print(f"Accuracy : {dt.score(x_test, y_test)}")
print(f"Cohen Kappa : {cohen_kappa_score(y_test, y_pred)}")
print(f"Classification Report :\n{classification_report(y_test, y_pred)}")
print("------------------------------------------------------------")


#--------------------------------------------------------------------------------------------------------------------------

"""## SVM"""

svm = SVC(C=100)
svm.fit(x_train, y_train)
y_pred =  svm.predict(x_test)
print("--------------------- SVM Report ---------------------")
print(f"Accuracy : {svm.score(x_test, y_test)}")
print(f"Cohen Kappa : {cohen_kappa_score(y_test, y_pred)}")
print(f"Classification Report :\n{classification_report(y_test, y_pred)}")
print("------------------------------------------------------")

#--------------------------------------------------------------------------------------------------------------------------

"""## Voting Classifier"""

estimator = []

#in voting classifire all togather and see score


# Logistic Regression
estimator.append(('Logistic  Regression', lr))

# Decision Tree
estimator.append(('Decision Tree', dt))

# Support Vector Machine
estimator.append(('SVM', svm))

# Define the Voting Classifier
#you can put weight according to your scroe 
vc = VotingClassifier(estimators=estimator, weights=[1, 1, 2])

eclf = vc.fit(x_train, y_train)
print("Score:{0}".format(eclf.score(x_train, y_train)))

y_pred = eclf.predict(x_test)
print("--------------------- Voting Classifier Report ---------------------")
print(f"Accuracy : {eclf.score(x_test, y_test)}")
print(f"Cohen Kappa : {cohen_kappa_score(y_test, y_pred)}")
print(f"Classification Report :\n{classification_report(y_test, y_pred)}")
print("--------------------------------------------------------------------")



#--------------------------------------------------------------------------------------------------------------------------

"""## Stacking classifier"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import *
from sklearn.preprocessing import *
from sklearn.metrics import *
from sklearn.svm import *
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import StackingClassifier
from sklearn import model_selection

# %matplotlib inline

import seaborn as sns 

np.random.seed(2021)

columns = ['letter', 'xbox', 'ybox', 'width', 'height', 
           'onpix', 'xbar', 'ybar','x2bar', 'y2bar', 
           'xybar', 'x2ybar', 'xy2bar', 'xedge', 'xedgey', 'yedge', 'yedgex']
           
df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/letter-recognition/letter-recognition.data', 
                 header=None, 
                 names = columns)


#--------------------------------------------------------------------------------------------------------------------------
#in Stacking classifier first i put 2 classifier 
#SVC and nuSVC , you cand add more classifier as well
estimators = [
              ('SVC', SVC(C = 100, gamma='auto', kernel='poly')),
              ('nuSVC', NuSVC(kernel='poly')),
]


#--------------------------------------------------------------------------------------------------------------------------
#then add StackingClassifier
#StackingClassifier add other classifier as well , we add RandomForestClassifier
sc = StackingClassifier(estimators=estimators, final_estimator=RandomForestClassifier())


#--------------------------------------------------------------------------------------------------------------------------

#in sc.fit(x_train, y_train) 3 classifier are run 1st SVC , 2nd nuSVC and 3rd and final RandomForestClassifier
sc.fit(x_train, y_train)
y_pred =  sc.predict(x_test)
print("--------------- SVM Stacking Classifier Report -----------------------")
print(f"Accuracy : {sc.score(x_test, y_test)}")
print(f"Cohen Kappa : {cohen_kappa_score(y_test, y_pred)}")
print(f"Classification Report :\n{classification_report(y_test, y_pred, target_names=np.unique(Y).tolist())}")
print("----------------------------------------------------------------------")


#--------------------------------------------------------------------------------------------------------------------------
print(y)

print(Y)

x = sc.predict([[2,5,6,1,0,6,3,6,5,5,5,2,3,10,5,6]])

print(x)



z = np.array(y)

print(z)

z.dtype

type(z)

count = 0
for i in y:
  if i== 3:
    break
  count = count+1

count

print(Y[count])

