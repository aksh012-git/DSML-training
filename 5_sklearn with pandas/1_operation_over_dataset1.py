# -*- coding: utf-8 -*-
"""DSMLpractical_2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18DaHwuW9XzBiB7JXzbatJPy0abxJkvxq
"""

#----dataset1 define man able to purchase Audi car or not------------------------------

#you want to perform some operations on your dataset than 
#step 1 - fill up or impute missing value
#step 2 - Handling categorical dataset like string and int and so many things

import pandas as pd
import numpy as np

dataset = pd.read_csv('dataset1.csv')

#-------------------------------------------------------------------------------------------------------------
#now devide our dataset into two part
x = dataset.iloc[:,:-1].values 
x
#x is our input and iloc[row,column] you write only ":" then this defined 0:n row or column
#you write :-1 then is defined 0:-1

#-------------------------------------------------------------------------------------------------------------
#y is our output
y = dataset.iloc[:,-1].values
y

#-------------------------------------------------------------------------------------------------------------
#what is Median
#Ex- 1,9,4,7,12,67,23,13
#stpe-1 make data in ascending order -->> 1,4,7,9,12,13,23,67
#find middle value -->> 9 and 12 then---> 9 + 12 = 21/2 = 10.5 is our Median
#Ex - 1,7,2,5,4,3,9
#step 1 ---> 1,2,3,4,5,7,9 --> 4 is Median

#-------------------------------------------------------------------------------------------------------------
#most frequent value
#Ex - 1,5,2,7,1,6,1,6,1,9,1 --->> most frequent value is 1

#-------------------------------------------------------------------------------------------------------------
#use for fill missing values in our dataset
from sklearn.impute import SimpleImputer
#step 1:read to fill missing value as "mean"
imputer = SimpleImputer(missing_values=np.nan,strategy='mean')
#step 2:which dataset and which place you apply this imputer and you can't use fit for string so i choose 1:3 column 
imputer = imputer.fit(x[:,1:3]) 
#step 3:transfer value nan to mean value and store 
x[:,1:3] = imputer.transform(x[:,1:3])
x

#see output NaN value fill by 'mean value'


#-------------------------------------------------------------------------------------------------------------
#you can see your x 0th column(country name) not a Numerical data,this a categorical data
#so, you want to transfer your categorical data to numerical data
#Three tpye of method use for transfer data set cat.. to num..
#LabelEncoder
#OneHotEncoder
#CountVectorizer


#-------------------------------------------------------------------------------------------------------------
#transfer cat.. to num..
from sklearn.preprocessing import LabelEncoder
lebelEncoder_of_x = LabelEncoder()
#fit and transfer perfrom both then write "fit_transfer"
x[:,0] = lebelEncoder_of_x.fit_transform(x[:,0])
x

#see o/p your data is encode with numerical 
#like France's F is first of A to Z then put 0
#like Germany's G is second of A to Z then put 1
#Spain's S is third of A to Z then put 2


#-------------------------------------------------------------------------------------------------------------
#i look at our x is input and y is output
#i want to split my data set into training and tesing part
#training -->> training means first i train my machine for give my expected output
#testing ---->> means i test my trainable output is occur or not

#devide my data 
from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state = 0)

#test_size = 0.2 means 20% data use for test and 80% data use for traing
#you can also define "train_size"

#-------------------------------------------------------------------------------------------------------------
x_train

x_test

y_train

y_test


#-------------------------------------------------------------------------------------------------------------
#1000000000 + 1 = 1000000001
#but 1000000000 is vary vary large value as compair to 1 so our machine remove this value 
#but some cases vary small value is vary important 
#so, you not niglat small value when large value is available then you use "Feature  Scaling"

#Feature Scaling--->
#1)MinMaxScaler
#2)Standard Scaler

#-------------------------------------------------------------------------------------------------------------
#MinMaxScaler--->
# our Data --> x = 2,4,1,6
""" 
Min_Max(2) = (x - xmin) / (xmax - xmin)
        = (2 - 1)/(6 - 1)
        = 0.2
Min_Max(4) = (4 - 1)/(6 - 1) = 0.6
Min_Max(1) = (1 - 1)/(6 - 1) = 0
Min_Max(6) = (6 - 1)/(6 - 1) = 1
"""

#-------------------------------------------------------------------------------------------------------------
#Standard Scaler--->
#our Data --> x = 2,4,1,6
"""
Standard Scaler = (x - u) / s

u ---> mean of x -->> (2+4+1+6)/4 = 3.25
s ---> standard deviation
s = https://www.google.com/search?q=standard+deviation&oq=standard+deviation&aqs=chrome..69i57j0i433j69i59l2j69i60l3.368j0j7&sourceid=chrome&ie=UTF-8
s = sqrt[(2-3.25)^2 + (1-3.25)^2 + (4-3.25)^2 + (6-3.25)^2 )/4]
"""

#-------------------------------------------------------------------------------------------------------------
#example of StandardScaler
from sklearn.preprocessing import StandardScaler
StandardScaler_of_x = StandardScaler()
x_train1 = StandardScaler_of_x.fit_transform(x_train)
x_test1 = StandardScaler_of_x.fit_transform(x_test)

#-------------------------------------------------------------------------------------------------------------
x_train1

x_test1


#-------------------------------------------------------------------------------------------------------------
#example of MinMaxScaler
from sklearn.preprocessing import MinMaxScaler
MinMaxScaler_of_x = MinMaxScaler()
x_train2 = MinMaxScaler_of_x.fit_transform(x_train)
x_test2 = MinMaxScaler_of_x.fit_transform(x_test)

#-------------------------------------------------------------------------------------------------------------
x_train2

x_test2

#-------------------------------------------------------------------------------------------------------------